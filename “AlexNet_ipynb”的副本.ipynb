{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVzIlKLqwIJQg+jcqO7hc5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wuyi010/AlexNet/blob/main/%E2%80%9CAlexNet_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1、torch.nn包中主要包含了用来搭建各个层的模块  （Modules），比如全连接、二维卷积、池化等；torch.nn包中还包含了一系列有用的loss函数，这些函数也是在训练神经网络时必不可少的，比如CrossEntropyLoss、MSELoss等；另外，torch.nn.functional子包中包含了常用的激活函数，如relu、leaky_relu、prelu、sigmoid等。\n",
        "\n",
        "2、torch.optim包则主要包含了用来更新参数的优化算法，比如SGD、AdaGrad、RMSProp、 Adam等。https://zhuanlan.zhihu.com/p/208178763"
      ],
      "metadata": {
        "id": "aWK0hUGmoHch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVZy3GU2kiTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "05f956b9-5dc5-4596-95ab-da8e5cd1bbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "sys.executable\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/bin/python3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import sys\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)   # cuda\n",
        "print(\"sys.executable\")\n",
        "sys.executable   # /usr/bin/python3\n",
        "# print(\"sys.modules\")\n",
        "# sys.modules    #注意区别sys.modules和sys.builtin_module_names——前者的关键字（keys）列出的是导入的模块名，而后者则是解释器内置的模块名。\n",
        "# print(\"sys.builtin_module_names\")\n",
        "# sys.builtin_module_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "数据下载"
      ],
      "metadata": {
        "id": "j7VrS6p0Zpoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'): # root：将数据保存在本地什么位置\n",
        "    if sys.platform.startswith('win'):\n",
        "        num_workers = 0\n",
        "    else:            # 将num_workers分配给数据加载程序的准则 https://www.cnblogs.com/hesse-summer/p/11343870.html\n",
        "        num_workers = 4 \n",
        "    trans = []\n",
        "    if resize:  # https://zhuanlan.zhihu.com/p/476220305\n",
        "        trans.append(torchvision.transforms.Resize(size=resize))# transforms.Compose([transforms.Resize(resize),transforms.ToTensor(),])\n",
        "    trans.append(torchvision.transforms.ToTensor()) # 转为Tensor类型\n",
        "\n",
        "    transform = torchvision.transforms.Compose(trans)  \n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    #训练数据集的加载器，自动将数据分割成batch，顺序随机打乱\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "              \n",
        "\n",
        "    return train_iter, test_iter\n",
        "\n",
        "batch_size = 128\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
      ],
      "metadata": {
        "id": "MFdGEZhXlp6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a175ac-96fa-4f09-aa2e-a87527747da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 15928370.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 284841.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4892157.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 3287496.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__() # 继承父类并使用父类初始化方法来初始化子类 \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2), # kernel_size, stride\n",
        "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
        "            nn.Conv2d(96, 256, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
        "            nn.Conv2d(256, 384, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 384, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256*5*5, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        feature = self.conv(img)\n",
        "        output = self.fc(feature.view(img.shape[0], -1))\n",
        "        return output\n",
        "\n",
        "net = AlexNet()\n",
        "\n",
        "def evaluate_accuracy(data_iter, net, device=None):\n",
        "    if device is None and isinstance(net, torch.nn.Module):\n",
        "        # 如果没指定device就使用net的device\n",
        "        device = list(net.parameters())[0].device\n",
        "    acc_sum, n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            net.eval() # 评估模式, 这会关闭dropout\n",
        "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
        "            net.train() # 改回训练模式\n",
        "            n += y.shape[0]\n",
        "    return acc_sum / n\n",
        "\n",
        "\n",
        "def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
        "    net = net.to(device)\n",
        "    print(\"training on \", device)\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            optimizer.zero_grad()\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            train_l_sum += l.cpu().item()\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
        "            n += y.shape[0]\n",
        "            batch_count += 1\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
        "\n"
      ],
      "metadata": {
        "id": "rEiUoo8IlyPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs = 0.001, 100\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAcl7t1tqy1J",
        "outputId": "15566594-ebb5-4b8e-e899-dad0f3f6981a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on  cuda\n",
            "epoch 1, loss 0.0706, train acc 0.979, test acc 0.919, time 57.4 sec\n",
            "epoch 2, loss 0.0730, train acc 0.979, test acc 0.910, time 57.7 sec\n",
            "epoch 3, loss 0.0685, train acc 0.979, test acc 0.916, time 57.5 sec\n",
            "epoch 4, loss 0.0882, train acc 0.975, test acc 0.914, time 57.3 sec\n",
            "epoch 5, loss 0.0644, train acc 0.981, test acc 0.916, time 58.4 sec\n",
            "epoch 6, loss 0.0561, train acc 0.983, test acc 0.916, time 57.2 sec\n",
            "epoch 7, loss 0.0654, train acc 0.980, test acc 0.913, time 58.5 sec\n",
            "epoch 8, loss 0.0704, train acc 0.979, test acc 0.914, time 57.1 sec\n",
            "epoch 9, loss 0.0568, train acc 0.983, test acc 0.918, time 58.3 sec\n",
            "epoch 10, loss 0.0614, train acc 0.982, test acc 0.915, time 57.1 sec\n",
            "epoch 11, loss 0.0657, train acc 0.981, test acc 0.915, time 58.5 sec\n",
            "epoch 12, loss 0.0625, train acc 0.981, test acc 0.916, time 57.0 sec\n",
            "epoch 13, loss 0.0888, train acc 0.976, test acc 0.913, time 57.9 sec\n",
            "epoch 14, loss 0.0604, train acc 0.981, test acc 0.917, time 57.4 sec\n",
            "epoch 15, loss 0.0524, train acc 0.984, test acc 0.911, time 57.5 sec\n",
            "epoch 16, loss 0.0613, train acc 0.982, test acc 0.917, time 57.8 sec\n",
            "epoch 17, loss 0.0670, train acc 0.980, test acc 0.916, time 57.3 sec\n",
            "epoch 18, loss 0.0649, train acc 0.981, test acc 0.917, time 58.5 sec\n",
            "epoch 19, loss 0.0617, train acc 0.982, test acc 0.916, time 57.3 sec\n",
            "epoch 20, loss 0.0539, train acc 0.984, test acc 0.914, time 58.5 sec\n",
            "epoch 21, loss 0.0550, train acc 0.983, test acc 0.912, time 57.4 sec\n",
            "epoch 22, loss 0.0592, train acc 0.983, test acc 0.913, time 58.4 sec\n",
            "epoch 23, loss 0.0703, train acc 0.980, test acc 0.912, time 57.1 sec\n",
            "epoch 24, loss 0.0638, train acc 0.981, test acc 0.917, time 58.5 sec\n",
            "epoch 25, loss 0.0575, train acc 0.983, test acc 0.913, time 57.2 sec\n",
            "epoch 26, loss 0.0649, train acc 0.981, test acc 0.916, time 58.6 sec\n",
            "epoch 27, loss 0.0525, train acc 0.985, test acc 0.914, time 57.3 sec\n",
            "epoch 28, loss 0.0418, train acc 0.987, test acc 0.917, time 58.3 sec\n",
            "epoch 29, loss 0.0747, train acc 0.979, test acc 0.913, time 57.1 sec\n",
            "epoch 30, loss 0.0538, train acc 0.983, test acc 0.912, time 58.0 sec\n",
            "epoch 31, loss 0.0859, train acc 0.976, test acc 0.912, time 57.8 sec\n",
            "epoch 32, loss 0.0735, train acc 0.979, test acc 0.919, time 57.5 sec\n",
            "epoch 33, loss 0.0669, train acc 0.980, test acc 0.913, time 57.5 sec\n",
            "epoch 34, loss 0.0492, train acc 0.985, test acc 0.908, time 57.1 sec\n",
            "epoch 35, loss 0.0647, train acc 0.983, test acc 0.915, time 57.9 sec\n",
            "epoch 36, loss 0.0541, train acc 0.984, test acc 0.915, time 57.6 sec\n",
            "epoch 37, loss 0.0662, train acc 0.981, test acc 0.905, time 58.1 sec\n",
            "epoch 38, loss 0.0661, train acc 0.981, test acc 0.915, time 57.8 sec\n",
            "epoch 39, loss 0.0606, train acc 0.984, test acc 0.915, time 57.9 sec\n",
            "epoch 40, loss 0.0680, train acc 0.981, test acc 0.915, time 57.6 sec\n",
            "epoch 41, loss 0.0623, train acc 0.983, test acc 0.917, time 58.3 sec\n",
            "epoch 42, loss 0.0793, train acc 0.979, test acc 0.910, time 57.8 sec\n",
            "epoch 43, loss 0.0543, train acc 0.984, test acc 0.915, time 57.9 sec\n",
            "epoch 44, loss 0.0635, train acc 0.982, test acc 0.910, time 57.5 sec\n",
            "epoch 45, loss 0.0503, train acc 0.986, test acc 0.916, time 58.1 sec\n",
            "epoch 46, loss 0.0640, train acc 0.982, test acc 0.915, time 58.1 sec\n",
            "epoch 47, loss 0.0703, train acc 0.981, test acc 0.913, time 58.2 sec\n",
            "epoch 48, loss 0.0570, train acc 0.984, test acc 0.918, time 57.8 sec\n",
            "epoch 49, loss 0.0795, train acc 0.978, test acc 0.916, time 56.3 sec\n",
            "epoch 50, loss 0.0612, train acc 0.983, test acc 0.910, time 56.8 sec\n",
            "epoch 51, loss 0.0630, train acc 0.983, test acc 0.917, time 57.7 sec\n",
            "epoch 52, loss 0.0472, train acc 0.987, test acc 0.914, time 56.9 sec\n",
            "epoch 53, loss 0.0586, train acc 0.984, test acc 0.917, time 56.4 sec\n",
            "epoch 54, loss 0.0647, train acc 0.982, test acc 0.917, time 56.8 sec\n",
            "epoch 55, loss 0.0749, train acc 0.980, test acc 0.917, time 57.0 sec\n",
            "epoch 56, loss 0.0602, train acc 0.984, test acc 0.917, time 56.4 sec\n",
            "epoch 57, loss 0.0657, train acc 0.983, test acc 0.915, time 56.5 sec\n",
            "epoch 58, loss 0.0726, train acc 0.980, test acc 0.917, time 56.9 sec\n",
            "epoch 59, loss 0.0709, train acc 0.981, test acc 0.919, time 56.3 sec\n",
            "epoch 60, loss 0.0675, train acc 0.981, test acc 0.914, time 56.7 sec\n",
            "epoch 61, loss 0.0638, train acc 0.983, test acc 0.916, time 57.1 sec\n",
            "epoch 62, loss 0.0454, train acc 0.987, test acc 0.913, time 56.3 sec\n",
            "epoch 63, loss 0.0540, train acc 0.986, test acc 0.915, time 56.9 sec\n",
            "epoch 64, loss 0.0627, train acc 0.983, test acc 0.919, time 56.9 sec\n",
            "epoch 65, loss 0.0741, train acc 0.980, test acc 0.908, time 56.5 sec\n",
            "epoch 66, loss 0.0615, train acc 0.983, test acc 0.915, time 56.5 sec\n",
            "epoch 67, loss 0.0631, train acc 0.984, test acc 0.916, time 57.0 sec\n",
            "epoch 68, loss 0.0656, train acc 0.983, test acc 0.917, time 56.9 sec\n",
            "epoch 69, loss 0.0721, train acc 0.981, test acc 0.916, time 56.1 sec\n",
            "epoch 70, loss 0.0565, train acc 0.984, test acc 0.920, time 56.7 sec\n",
            "epoch 71, loss 0.0381, train acc 0.990, test acc 0.917, time 56.9 sec\n",
            "epoch 72, loss 0.0666, train acc 0.982, test acc 0.916, time 56.1 sec\n",
            "epoch 73, loss 0.0961, train acc 0.975, test acc 0.914, time 56.4 sec\n",
            "epoch 74, loss 0.0812, train acc 0.977, test acc 0.913, time 56.9 sec\n",
            "epoch 75, loss 0.0618, train acc 0.983, test acc 0.918, time 56.2 sec\n",
            "epoch 76, loss 0.0572, train acc 0.985, test acc 0.917, time 56.7 sec\n",
            "epoch 77, loss 0.0352, train acc 0.990, test acc 0.918, time 56.9 sec\n",
            "epoch 78, loss 0.0570, train acc 0.985, test acc 0.919, time 56.0 sec\n",
            "epoch 79, loss 0.0922, train acc 0.977, test acc 0.912, time 56.4 sec\n",
            "epoch 80, loss 0.0717, train acc 0.981, test acc 0.919, time 57.4 sec\n",
            "epoch 81, loss 0.0695, train acc 0.982, test acc 0.911, time 56.4 sec\n",
            "epoch 82, loss 0.0667, train acc 0.983, test acc 0.914, time 56.7 sec\n",
            "epoch 83, loss 0.0473, train acc 0.988, test acc 0.918, time 56.8 sec\n",
            "epoch 84, loss 0.1115, train acc 0.972, test acc 0.914, time 56.2 sec\n",
            "epoch 85, loss 0.0642, train acc 0.983, test acc 0.915, time 56.9 sec\n",
            "epoch 86, loss 0.0787, train acc 0.980, test acc 0.915, time 57.1 sec\n",
            "epoch 87, loss 0.0476, train acc 0.987, test acc 0.904, time 56.6 sec\n",
            "epoch 88, loss 0.0719, train acc 0.980, test acc 0.918, time 56.6 sec\n",
            "epoch 89, loss 0.0516, train acc 0.987, test acc 0.918, time 56.8 sec\n",
            "epoch 90, loss 0.0490, train acc 0.988, test acc 0.916, time 56.6 sec\n",
            "epoch 91, loss 0.0677, train acc 0.983, test acc 0.914, time 56.2 sec\n",
            "epoch 92, loss 0.0820, train acc 0.979, test acc 0.867, time 56.9 sec\n",
            "epoch 93, loss 0.1093, train acc 0.968, test acc 0.919, time 57.2 sec\n",
            "epoch 94, loss 0.0430, train acc 0.989, test acc 0.913, time 56.7 sec\n",
            "epoch 95, loss 0.0716, train acc 0.981, test acc 0.913, time 57.1 sec\n",
            "epoch 96, loss 0.0572, train acc 0.985, test acc 0.918, time 57.8 sec\n",
            "epoch 97, loss 0.0563, train acc 0.986, test acc 0.919, time 56.8 sec\n",
            "epoch 98, loss 0.0961, train acc 0.975, test acc 0.915, time 56.9 sec\n",
            "epoch 99, loss 0.0688, train acc 0.983, test acc 0.920, time 57.5 sec\n",
            "epoch 100, loss 0.0542, train acc 0.987, test acc 0.917, time 58.3 sec\n"
          ]
        }
      ]
    }
  ]
}