{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFOboQTx1Xfs5iqiEpdozI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wuyi010/AlexNet/blob/main/%E2%80%9CAlexNet_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet配置依赖项\n",
        "1、torch.nn包中主要包含了用来搭建各个层的模块  （Modules），比如全连接、二维卷积、池化等；torch.nn包中还包含了一系列有用的loss函数，这些函数也是在训练神经网络时必不可少的，比如CrossEntropyLoss、MSELoss等；另外，torch.nn.functional子包中包含了常用的激活函数，如relu、leaky_relu、prelu、sigmoid等。\n",
        "\n",
        "2、torch.optim包则主要包含了用来更新参数的优化算法，比如SGD、AdaGrad、RMSProp、 Adam等。https://zhuanlan.zhihu.com/p/208178763"
      ],
      "metadata": {
        "id": "aWK0hUGmoHch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iVZy3GU2kiTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6790f47a-a977-40a4-f853-b0db7d941395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "sys.executable\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/bin/python3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import sys\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)   # cuda\n",
        "print(\"sys.executable\")\n",
        "sys.executable   # /usr/bin/python3\n",
        "# print(\"sys.modules\")\n",
        "# sys.modules    #注意区别sys.modules和sys.builtin_module_names——前者的关键字（keys）列出的是导入的模块名，而后者则是解释器内置的模块名。\n",
        "# print(\"sys.builtin_module_names\")\n",
        "# sys.builtin_module_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet数据下载"
      ],
      "metadata": {
        "id": "j7VrS6p0Zpoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'): # root：将数据保存在本地什么位置\n",
        "    if sys.platform.startswith('win'):\n",
        "        num_workers = 0\n",
        "    else:            # 将num_workers分配给数据加载程序的准则 https://www.cnblogs.com/hesse-summer/p/11343870.html\n",
        "        num_workers = 4 \n",
        "    trans = []\n",
        "    if resize:  # https://zhuanlan.zhihu.com/p/476220305\n",
        "        trans.append(torchvision.transforms.Resize(size=resize))# transforms.Compose([transforms.Resize(resize),transforms.ToTensor(),])\n",
        "    trans.append(torchvision.transforms.ToTensor()) # 转为Tensor类型\n",
        "\n",
        "    transform = torchvision.transforms.Compose(trans)  \n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    #训练数据集的加载器，自动将数据分割成batch，顺序随机打乱\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "              \n",
        "\n",
        "    return train_iter, test_iter\n",
        "\n",
        "batch_size = 128\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
      ],
      "metadata": {
        "id": "MFdGEZhXlp6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a175ac-96fa-4f09-aa2e-a87527747da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 15928370.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 284841.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4892157.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 3287496.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet模型\n",
        "https://blog.csdn.net/qq_27825451/article/details/90551513\n",
        "\n",
        "```\n",
        "class Sequential(Module):#继承Module  `\n",
        "    def __init__(self, *args):  # 重写了构造函数\n",
        "    def _get_item_by_idx(self, iterator, idx)：\n",
        "    def __getitem__(self, idx):\n",
        "    def __setitem__(self, idx, module):\n",
        "    def __delitem__(self, idx):\n",
        "    def __len__(self):\n",
        "    def __dir__(self):\n",
        "    def forward(self, input):  # 重写关键方法forward\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "3VKqtM4swXX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__() # 继承父类并使用父类初始化方法来初始化子类 \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2), # kernel_size, stride\n",
        "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
        "            nn.Conv2d(96, 256, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
        "            nn.Conv2d(256, 384, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 384, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256*5*5, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        feature = self.conv(img)\n",
        "        output = self.fc(feature.view(img.shape[0], -1))\n",
        "        return output"
      ],
      "metadata": {
        "id": "IgeFYHyYwTUs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet实例化"
      ],
      "metadata": {
        "id": "_72o-4EqzeJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = AlexNet()\n",
        "print(net)\n",
        "print(\"---------------------------------------------------\")\n",
        "print(net.__dict__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d6pwWtBwUqM",
        "outputId": "5f56d376-ce22-4b2a-c00d-ac0cd71f6f9b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU()\n",
            "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU()\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "---------------------------------------------------\n",
            "{'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('conv', Sequential(\n",
            "  (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): ReLU()\n",
            "  (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): ReLU()\n",
            "  (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU()\n",
            "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")), ('fc', Sequential(\n",
            "  (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "))])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CrossEntropyLoss()函数"
      ],
      "metadata": {
        "id": "_-yyGIEgNQhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(net.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OuCLVEkVbn1",
        "outputId": "210d247b-fd07-4ac3-bff4-6fe0c9c1935b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7f03915cde70>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch中CrossEntropyLoss()函数的主要是将softmax-log-NLLLoss合并到一块得到的结果。\n",
        "\n",
        "1、Softmax后的数值都在0~1之间，所以ln之后值域是负无穷到0。\n",
        "\n",
        "2、然后将Softmax之后的结果取log，将乘法改成加法减少计算量，同时保障函数的单调性。\n",
        "\n",
        "3、NLLLoss的结果就是把上面的输出与Label对应的那个值拿出来(下面例子中就是：将log_output\\logsoftmax_output中与y_target对应的值拿出来)，去掉负号，再求均值。下面是例子：https://zhuanlan.zhihu.com/p/98785902"
      ],
      "metadata": {
        "id": "U3551A-rNGL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "x_input=torch.randn(3,3)#随机生成输入 \n",
        "y_target=torch.tensor([1,2,0])#设置输出具体值 print('y_target\\n',y_target)\n",
        "print('x_input:\\n',x_input)\n",
        "print('y_target:\\n',y_target)\n",
        "print(\"_______________________________________________________________________\")\n",
        "\n",
        "#计算输入softmax，此时可以看到每一行加到一起结果都是1\n",
        "softmax_func=nn.Softmax(dim=1)\n",
        "soft_output=softmax_func(x_input)\n",
        "print('soft_output:\\n',soft_output)\n",
        "\n",
        "#在softmax的基础上取log\n",
        "log_output=torch.log(soft_output)\n",
        "print('log_output:\\n',log_output)\n",
        "\n",
        "#对比softmax与log的结合与nn.LogSoftmaxloss(负对数似然损失)的输出结果，发现两者是一致的。\n",
        "logsoftmax_func=nn.LogSoftmax(dim=1)\n",
        "logsoftmax_output=logsoftmax_func(x_input)\n",
        "print('logsoftmax_output:\\n',logsoftmax_output)\n",
        "\n",
        "#pytorch中关于NLLLoss的默认参数配置为：reducetion=True、size_average=True\n",
        "nllloss_func=nn.NLLLoss()\n",
        "nlloss_output=nllloss_func(logsoftmax_output,y_target)\n",
        "print('nlloss_output:\\n',nlloss_output)\n",
        "\n",
        "#直接使用pytorch中的loss_func=nn.CrossEntropyLoss()看与经过NLLLoss的计算是不是一样\n",
        "crossentropyloss=nn.CrossEntropyLoss()\n",
        "crossentropyloss_output=crossentropyloss(x_input,y_target)\n",
        "print('crossentropyloss_output:\\n',crossentropyloss_output)\n",
        "#通过上面的结果可以看出，直接使用pytorch中的loss_func=nn.CrossEntropyLoss()\n",
        "#计算得到的结果与softmax-log-NLLLoss计算得到的结果是一致的。"
      ],
      "metadata": {
        "id": "yfKrc2goNFsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet准确率评估"
      ],
      "metadata": {
        "id": "8WQuoWbqUu9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_iter = test_iter\n",
        "def evaluate_accuracy(data_iter, net, device=None):\n",
        "    if device is None and isinstance(net, torch.nn.Module):\n",
        "        # 如果没指定device就使用net的device\n",
        "        device = list(net.parameters())[0].device\n",
        "    acc_sum, n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            net.eval() # 评估模式, 这会关闭dropout\n",
        "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
        "            net.train() # 改回训练模式\n",
        "            n += y.shape[0] #行数\n",
        "    return acc_sum / n"
      ],
      "metadata": {
        "id": "due7WLMKU2W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet模型训练"
      ],
      "metadata": {
        "id": "T_dNi8ODU5eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "多个GPU指定计算资源\n",
        "```\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "model = Model() \n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model，device_ids=[0,1,2])   \n",
        "    model.to(device)\n",
        "```"
      ],
      "metadata": {
        "id": "-vxO8SWZYf5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CrossEntropyLoss:交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性——判定实际的输出与期望的输出的接近程度"
      ],
      "metadata": {
        "id": "gDi7CxCAtDJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
        "    net = net.to(device) #显示指定需要使用的计算资源:读取数据的tensor变量copy一份到device所指定的GPU上去\n",
        "    print(\"training on \", device)\n",
        "    loss = torch.nn.CrossEntropyLoss() #交叉熵损失函数\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y_hat = net(X)\n",
        "            lossValue = loss(y_hat, y)\n",
        "            optimizer.zero_grad()  #清空过往梯度\n",
        "            lossValue.backward()  #反向传播，计算当前梯度\n",
        "            optimizer.step()    #根据梯度更新网络参数\n",
        "            train_lossValue_sum += lossValue.cpu().item()\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item() #https://blog.csdn.net/AMBE_R_/article/details/115512034\n",
        "            n += y.shape[0]\n",
        "            batch_count += 1\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "         % (epoch+1,train_lossValue_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
        "        "
      ],
      "metadata": {
        "id": "rEiUoo8IlyPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "yf_hhoTWVum-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs = 0.001, 3\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr) #优化器\n",
        "train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "NAcl7t1tqy1J",
        "outputId": "18618882-1f00-4c43-db25-45496af27239"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cffeb1a2ef56>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#优化器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training on  cuda\n",
        "epoch 1, loss 0.0706, train acc 0.979, test acc 0.919, time 57.4 sec\n",
        "\n",
        "epoch 2, loss 0.0730, train acc 0.979, test acc 0.910, time 57.7 sec\n",
        "\n",
        "epoch 3, loss 0.0685, train acc 0.979, test acc 0.916, time 57.5 sec\n",
        "\n",
        "epoch 4, loss 0.0882, train acc 0.975, test acc 0.914, time 57.3 sec\n",
        "\n",
        "epoch 5, loss 0.0644, train acc 0.981, test acc 0.916, time 58.4 sec\n",
        "\n",
        "epoch 6, loss 0.0561, train acc 0.983, test acc 0.916, time 57.2 sec\n",
        "\n",
        "epoch 7, loss 0.0654, train acc 0.980, test acc 0.913, time 58.5 sec\n",
        "\n",
        "epoch 8, loss 0.0704, train acc 0.979, test acc 0.914, time 57.1 sec\n",
        "\n",
        "epoch 9, loss 0.0568, train acc 0.983, test acc 0.918, time 58.3 sec\n",
        "\n",
        "epoch 10, loss 0.0614, train acc 0.982, test acc 0.915, time 57.1 sec"
      ],
      "metadata": {
        "id": "t3TM1llVUkU4"
      }
    }
  ]
}