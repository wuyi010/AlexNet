{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j7VrS6p0Zpoj",
        "3VKqtM4swXX_",
        "_72o-4EqzeJ1",
        "_-yyGIEgNQhL"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSHhPrVarMIq2pmYB/cSe7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wuyi010/AlexNet/blob/main/AlexNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet配置依赖项\n",
        "1、torch.nn包中主要包含了用来搭建各个层的模块  （Modules），比如全连接、二维卷积、池化等；torch.nn包中还包含了一系列有用的loss函数，这些函数也是在训练神经网络时必不可少的，比如CrossEntropyLoss、MSELoss等；另外，torch.nn.functional子包中包含了常用的激活函数，如relu、leaky_relu、prelu、sigmoid等。\n",
        "\n",
        "2、torch.optim包则主要包含了用来更新参数的优化算法，比如SGD、AdaGrad、RMSProp、 Adam等。https://zhuanlan.zhihu.com/p/208178763"
      ],
      "metadata": {
        "id": "aWK0hUGmoHch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVZy3GU2kiTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4d008d28-d4e6-4440-f61c-eccc2d947c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "sys.executable\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/bin/python3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import sys\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)   # cuda\n",
        "print(\"sys.executable\")\n",
        "sys.executable   # /usr/bin/python3\n",
        "# print(\"sys.modules\")\n",
        "# sys.modules    #注意区别sys.modules和sys.builtin_module_names——前者的关键字（keys）列出的是导入的模块名，而后者则是解释器内置的模块名。\n",
        "# print(\"sys.builtin_module_names\")\n",
        "# sys.builtin_module_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet数据下载"
      ],
      "metadata": {
        "id": "j7VrS6p0Zpoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'): # root：将数据保存在本地什么位置\n",
        "    if sys.platform.startswith('win'):\n",
        "        num_workers = 0\n",
        "    else:            # 将num_workers分配给数据加载程序的准则 https://www.cnblogs.com/hesse-summer/p/11343870.html\n",
        "        num_workers = 4 \n",
        "    trans = []\n",
        "    if resize:  # https://zhuanlan.zhihu.com/p/476220305\n",
        "        trans.append(torchvision.transforms.Resize(size=resize))# transforms.Compose([transforms.Resize(resize),transforms.ToTensor(),])\n",
        "    trans.append(torchvision.transforms.ToTensor()) # 转为Tensor类型\n",
        "\n",
        "    transform = torchvision.transforms.Compose(trans)  \n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    #训练数据集的加载器，自动将数据分割成batch，顺序随机打乱\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "              \n",
        "\n",
        "    return train_iter, test_iter\n",
        "\n",
        "batch_size = 128\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
      ],
      "metadata": {
        "id": "MFdGEZhXlp6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d433a9-2b8e-4597-eec9-70ff6adc939b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8450862.31it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 147120.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2670733.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 20662466.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/Datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet模型\n",
        "https://blog.csdn.net/qq_27825451/article/details/90551513\n",
        "\n",
        "```\n",
        "class Sequential(Module):#继承Module  `\n",
        "    def __init__(self, *args):  # 重写了构造函数\n",
        "    def _get_item_by_idx(self, iterator, idx)：\n",
        "    def __getitem__(self, idx):\n",
        "    def __setitem__(self, idx, module):\n",
        "    def __delitem__(self, idx):\n",
        "    def __len__(self):\n",
        "    def __dir__(self):\n",
        "    def forward(self, input):  # 重写关键方法forward\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "3VKqtM4swXX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__() # 继承父类并使用父类初始化方法来初始化子类 \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2), # kernel_size, stride\n",
        "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
        "            nn.Conv2d(96, 256, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
        "            nn.Conv2d(256, 384, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 384, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256*5*5, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        feature = self.conv(img)\n",
        "        output = self.fc(feature.view(img.shape[0], -1))\n",
        "        return output"
      ],
      "metadata": {
        "id": "IgeFYHyYwTUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet实例化"
      ],
      "metadata": {
        "id": "_72o-4EqzeJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = AlexNet()\n",
        "print(net)\n",
        "print(\"---------------------------------------------------\")\n",
        "print(net.__dict__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d6pwWtBwUqM",
        "outputId": "65eb9614-2992-4482-eed8-07d10efff958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU()\n",
            "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU()\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "---------------------------------------------------\n",
            "{'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('conv', Sequential(\n",
            "  (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): ReLU()\n",
            "  (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): ReLU()\n",
            "  (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU()\n",
            "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")), ('fc', Sequential(\n",
            "  (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "))])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CrossEntropyLoss()函数"
      ],
      "metadata": {
        "id": "_-yyGIEgNQhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(net.parameters())"
      ],
      "metadata": {
        "id": "3OuCLVEkVbn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch中CrossEntropyLoss()函数的主要是将softmax-log-NLLLoss合并到一块得到的结果。\n",
        "\n",
        "1、Softmax后的数值都在0~1之间，所以ln之后值域是负无穷到0。\n",
        "\n",
        "2、然后将Softmax之后的结果取log，将乘法改成加法减少计算量，同时保障函数的单调性。\n",
        "\n",
        "3、NLLLoss的结果就是把上面的输出与Label对应的那个值拿出来(下面例子中就是：将log_output\\logsoftmax_output中与y_target对应的值拿出来)，去掉负号，再求均值。下面是例子：https://zhuanlan.zhihu.com/p/98785902"
      ],
      "metadata": {
        "id": "U3551A-rNGL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "x_input=torch.randn(3,3)#随机生成输入 \n",
        "y_target=torch.tensor([1,2,0])#设置输出具体值 print('y_target\\n',y_target)\n",
        "print('x_input:\\n',x_input)\n",
        "print('y_target:\\n',y_target)\n",
        "print(\"_______________________________________________________________________\")\n",
        "\n",
        "#计算输入softmax，此时可以看到每一行加到一起结果都是1\n",
        "softmax_func=nn.Softmax(dim=1)\n",
        "soft_output=softmax_func(x_input)\n",
        "print('soft_output:\\n',soft_output)\n",
        "\n",
        "#在softmax的基础上取log\n",
        "log_output=torch.log(soft_output)\n",
        "print('log_output:\\n',log_output)\n",
        "\n",
        "#对比softmax与log的结合与nn.LogSoftmaxloss(负对数似然损失)的输出结果，发现两者是一致的。\n",
        "logsoftmax_func=nn.LogSoftmax(dim=1)\n",
        "logsoftmax_output=logsoftmax_func(x_input)\n",
        "print('logsoftmax_output:\\n',logsoftmax_output)\n",
        "\n",
        "#pytorch中关于NLLLoss的默认参数配置为：reducetion=True、size_average=True\n",
        "nllloss_func=nn.NLLLoss()\n",
        "nlloss_output=nllloss_func(logsoftmax_output,y_target)\n",
        "print('nlloss_output:\\n',nlloss_output)\n",
        "\n",
        "#直接使用pytorch中的loss_func=nn.CrossEntropyLoss()看与经过NLLLoss的计算是不是一样\n",
        "crossentropyloss=nn.CrossEntropyLoss()\n",
        "crossentropyloss_output=crossentropyloss(x_input,y_target)\n",
        "print('crossentropyloss_output:\\n',crossentropyloss_output)\n",
        "#通过上面的结果可以看出，直接使用pytorch中的loss_func=nn.CrossEntropyLoss()\n",
        "#计算得到的结果与softmax-log-NLLLoss计算得到的结果是一致的。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfKrc2goNFsP",
        "outputId": "940e8652-c355-40ca-cac2-2b7c342d3cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_input:\n",
            " tensor([[-2.3374, -1.4258,  0.4562],\n",
            "        [-1.8598,  0.0224,  1.4308],\n",
            "        [-1.3683,  0.3601, -0.2085]])\n",
            "y_target:\n",
            " tensor([1, 2, 0])\n",
            "_______________________________________________________________________\n",
            "soft_output:\n",
            " tensor([[0.0504, 0.1255, 0.8241],\n",
            "        [0.0290, 0.1908, 0.7802],\n",
            "        [0.1018, 0.5734, 0.3247]])\n",
            "log_output:\n",
            " tensor([[-2.9872, -2.0755, -0.1935],\n",
            "        [-3.5389, -1.6567, -0.2482],\n",
            "        [-2.2845, -0.5561, -1.1247]])\n",
            "logsoftmax_output:\n",
            " tensor([[-2.9872, -2.0755, -0.1935],\n",
            "        [-3.5389, -1.6567, -0.2482],\n",
            "        [-2.2845, -0.5561, -1.1247]])\n",
            "nlloss_output:\n",
            " tensor(1.5361)\n",
            "crossentropyloss_output:\n",
            " tensor(1.5361)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet准确率评估"
      ],
      "metadata": {
        "id": "8WQuoWbqUu9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_iter = test_iter\n",
        "def evaluate_accuracy(data_iter, net, device=None):\n",
        "    if device is None and isinstance(net, torch.nn.Module):\n",
        "        # 如果没指定device就使用net的device\n",
        "        device = list(net.parameters())[0].device\n",
        "    acc_sum, n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            net.eval() # 评估模式, 这会关闭dropout\n",
        "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
        "            net.train() # 改回训练模式\n",
        "            n += y.shape[0] #行数\n",
        "    return acc_sum / n"
      ],
      "metadata": {
        "id": "due7WLMKU2W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet模型训练"
      ],
      "metadata": {
        "id": "T_dNi8ODU5eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "多个GPU指定计算资源\n",
        "```\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "model = Model() \n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model，device_ids=[0,1,2])   \n",
        "    model.to(device)\n",
        "```"
      ],
      "metadata": {
        "id": "-vxO8SWZYf5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CrossEntropyLoss:交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性——判定实际的输出与期望的输出的接近程度"
      ],
      "metadata": {
        "id": "gDi7CxCAtDJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
        "    net = net.to(device) #显示指定需要使用的计算资源:读取数据的tensor变量copy一份到device所指定的GPU上去\n",
        "    print(\"training on \", device)\n",
        "    loss = torch.nn.CrossEntropyLoss() #交叉熵损失函数\n",
        "    for epoch in range(num_epochs):\n",
        "        train_lossVal_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y_hat = net(X)\n",
        "            lossValue = loss(y_hat, y)\n",
        "            optimizer.zero_grad()  #清空过往梯度\n",
        "            lossValue.backward()  #反向传播，计算当前梯度\n",
        "            optimizer.step()    #根据梯度更新网络参数\n",
        "            train_lossVal_sum += lossValue.cpu().item()\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item() #https://blog.csdn.net/AMBE_R_/article/details/115512034\n",
        "            n += y.shape[0]\n",
        "            batch_count += 1\n",
        "\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' % (epoch+1,train_lossVal_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
        "        print('batch_count:%d, y.shape[0]:%d, batch_count:%d' %(batch_count,y.shape[0],batch_count))\n",
        "        "
      ],
      "metadata": {
        "id": "rEiUoo8IlyPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "yf_hhoTWVum-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs = 0.001, 5\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr) #优化器\n",
        "train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAcl7t1tqy1J",
        "outputId": "c495ac40-58bc-49b6-80ff-04cf37c9a115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on  cuda\n",
            "epoch 1, loss 0.6119, train acc 0.768, test acc 0.864, time 63.9 sec\n",
            "batch_count:469, y.shape[0]:96, batch_count:469\n",
            "epoch 2, loss 0.3280, train acc 0.877, test acc 0.870, time 58.6 sec\n",
            "batch_count:469, y.shape[0]:96, batch_count:469\n",
            "epoch 3, loss 0.2794, train acc 0.897, test acc 0.885, time 57.7 sec\n",
            "batch_count:469, y.shape[0]:96, batch_count:469\n",
            "epoch 4, loss 0.2562, train acc 0.905, test acc 0.902, time 59.2 sec\n",
            "batch_count:469, y.shape[0]:96, batch_count:469\n",
            "epoch 5, loss 0.2314, train acc 0.915, test acc 0.909, time 57.8 sec\n",
            "batch_count:469, y.shape[0]:96, batch_count:469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet结果\n",
        "epoch 1, loss 0.0706, train acc 0.979, test acc 0.919, time 57.4 sec\n",
        "\n",
        "epoch 2, loss 0.0730, train acc 0.979, test acc 0.910, time 57.7 sec\n",
        "\n",
        "epoch 3, loss 0.0685, train acc 0.979, test acc 0.916, time 57.5 sec\n",
        "\n",
        "epoch 4, loss 0.0882, train acc 0.975, test acc 0.914, time 57.3 sec\n",
        "\n",
        "epoch 5, loss 0.0644, train acc 0.981, test acc 0.916, time 58.4 sec\n",
        "\n",
        "epoch 6, loss 0.0561, train acc 0.983, test acc 0.916, time 57.2 sec\n",
        "\n",
        "epoch 7, loss 0.0654, train acc 0.980, test acc 0.913, time 58.5 sec\n",
        "\n",
        "epoch 8, loss 0.0704, train acc 0.979, test acc 0.914, time 57.1 sec\n",
        "\n",
        "epoch 9, loss 0.0568, train acc 0.983, test acc 0.918, time 58.3 sec\n",
        "\n",
        "epoch 10, loss 0.0614, train acc 0.982, test acc 0.915, time 57.1 sec"
      ],
      "metadata": {
        "id": "t3TM1llVUkU4"
      }
    }
  ]
}